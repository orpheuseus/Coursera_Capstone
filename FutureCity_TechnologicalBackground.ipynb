{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future city - background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now come to the background of this idea. The first idea was to use the 100 largest cities in the world, pick them from wikipedia, start with a Pandas DataFrame, clean the data and augment it with data from different providers. Unfortunately, it turned out in a research step, that not all relevant data is stored for all cities in the expected way. Therefore, we decided to concentrate on cities in US, Canada and Europe. Problems appear for most countries south of US and Australia. In order to solve these issues, a different dataset should be used, but wasn't found. For south asian countries similar problems appear but may be solved by further investigation of translations of the character encodings. In other words, in order to do all operations automated and not by hand, it is hard to find unique names for all cities that all data providers link to the same object (i.e. city)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps of the project are the following:\n",
    "\n",
    "1. <b>Data aggregation</b>\n",
    "\n",
    "    a) We take a list of all cities with more than 1 million inhabitants in US, Canada and Europe from Wikipedia, e.g. https://en.wikipedia.org/wiki/List_of_North_American_cities_by_population (in case these are too many requests in the later steps, we have to shrink down the number of cities)\n",
    "\n",
    "    b) Nominatim of OSM allows us via json to catch the center coordinates and urban area polygons for the cities. A first idea was to use the database of geonames at https://download.geonames.org/export/dump/ in order to get all the postal code areas of these places together with coordinates and then merge them, but the names in wikipedia have to be translated into native languages (and \"New York City\" in Geonames is \"New York\").\n",
    "\n",
    "    c) We explore all these areas in Foursquare. The account limits to 950.000 standard calls, which can be a search on coordinates. The answers include coordinates, so we can generate a hexagonal grid and get all the values. If there is a significant number of issues with missing coordinates we can replace this by a triangular grid and triangulate all the venues.</li>\n",
    "</ol>\n",
    "    \n",
    "    \n",
    "2. <b>Data exploration</b>\n",
    "\n",
    "    a) The next part of the idea is to generate a kind of fingerprint for each city by information that can be visualized in histograms and other graphs. E.g. the density of restaurants depending on the distance to the center. Of course, another problem arises here which is that some cities might have a different number of \"centers\", but this might be represented by the fingerprint. And sure, distances are given due to a certain metric which is related to topology, infrastructure, no-go areas and so on (but this is out of the scope of this project). The different centers can be found using k-means and categorized to small, medium, large.\n",
    "\n",
    "    b) Probably a spring model is a good approach to compare and cluster all the cities by the high-dimensional amount of data which, of course, needs to be normalized. The result can be plotted in 2d (a high-dimensional version of k-means is probably not doing a good job, but can be tried for fun).\n",
    "\n",
    "    c) What are the differences between the cities? Is there a way to order them according to some dynamics, which shows their future classes with certain probabilities? This will be done properly, as soon as enough structure is found in the data. \n",
    "\n",
    "\n",
    "3. <b>Decision making based on data</b>\n",
    "\n",
    "    a) We can learn from other cities about future evolution. And of course, additional information on all levels would be quite helpful to get a better ordering, but this is only a small project ;-) e.g. fingerprints of areas could be correlated to crime rates and types,....\n",
    "\n",
    "    b) If the decision is made, towards which direction a city should move, we can make profit out of the differences. First of all, we have to option to move towards a city type that isn't existing yet. Second, moving towards an existing city doesn't mean we want to copy this city, we only copy certain characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S.: you may wonder that all points are enumerated as 1. I wonder as well. In my local file this is numbered correctly, but not on GitHub. If you have any clue, please tell me! If I use html code for an enumeration via < o l > and < l i > the font is changed, the same happens if I remove the numbers. I don't want to enter CSS or add < f o n t > tags in order to bypass this problem, I would prefer to take an easy and straightforward solution,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
